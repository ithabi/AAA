{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ithabi/AAA/blob/main/quizzes/Wrap_up_quiz_Ensemble_Method_to_Students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 1 : c)\n",
        "\n",
        "QUESTION 2 : c)\n",
        "\n",
        "QUESTION 3 : b) e)\n",
        "\n",
        "QUESTION 4 : b) c)\n",
        "\n",
        "QUESTION 5 :\n",
        "\n",
        "QUESTION 6 :  "
      ],
      "metadata": {
        "id": "rgFOPNRRAQeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_validate, validation_curve\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
        "\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/bilals/scikit-learn-mooc/main/datasets/penguins.csv\")\n",
        "\n",
        "feature_names = [\n",
        "    \"Culmen Length (mm)\",\n",
        "    \"Culmen Depth (mm)\",\n",
        "    \"Flipper Length (mm)\",\n",
        "]\n",
        "target_name = \"Body Mass (g)\"\n",
        "\n",
        "dataset = dataset[feature_names + [target_name]].dropna(axis=\"rows\", how=\"any\")\n",
        "dataset = dataset.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "data, target = dataset[feature_names], dataset[target_name]"
      ],
      "metadata": {
        "id": "vBkYJZRGVBRB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap-up quiz - Ensemble Method\n",
        "\n",
        "**This quiz requires some programming to be answered.**\n",
        "\n",
        "This wrap-up quiz uses the penguins dataset, but notice that **we do not use the\n",
        "traditional `Species` column** as predictive target:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/bilals/scikit-learn-mooc/main/datasets/penguins.csv\")\n",
        "\n",
        "feature_names = [\n",
        "    \"Culmen Length (mm)\",\n",
        "    \"Culmen Depth (mm)\",\n",
        "    \"Flipper Length (mm)\",\n",
        "]\n",
        "target_name = \"Body Mass (g)\"\n",
        "\n",
        "dataset = dataset[feature_names + [target_name]].dropna(axis=\"rows\", how=\"any\")\n",
        "dataset = dataset.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "data, target = dataset[feature_names], dataset[target_name]\n",
        "```\n",
        "\n",
        "We therefore define our problem as a regression problem: we want to predict the\n",
        "body mass of a penguin given its culmen and flipper measurements.\n",
        "\n",
        "Notice that we randomly shuffled the rows of the dataset after loading it\n",
        "(`dataset.sample(frac=1, random_state=0)`). The reason is to break a spurious\n",
        "order-related statistical dependency that would otherwise cause trouble with the\n",
        "naive cross-validation procedure we use in this notebook. The problem of\n",
        "order-dependent samples will be discussed more in detail on the model evaluation\n",
        "module and is outside of the scope of this quiz for now.\n",
        "Now, evaluate the following tree-based models:\n",
        "\n",
        "- a decision tree regressor, i.e. `sklearn.tree.DecisionTreeRegressor`\n",
        "- a random forest regressor, i.e. `sklearn.ensemble.RandomForestRegressor`\n",
        "\n",
        "Use the default hyper-parameter settings for both models. The only exception\n",
        "is to pass `random_state=0` for all models to be sure to recover the exact\n",
        "same performance scores as the solutions to this quiz.\n",
        "\n",
        "Evaluate the generalization performance of these models using a 10-fold\n",
        "cross-validation:\n",
        "\n",
        "- use `sklearn.model_selection.cross_validate` to run the cross-validation routine\n",
        "- set the parameter `cv=10` to use a 10-fold cross-validation strategy. Store the\n",
        "training score of the cross-validation by setting the parameter\n",
        "`return_train_score=True` in the function `cross_validate`\n",
        "as we will use it later on.\n",
        "\n",
        "## Question 1\n",
        "```\n",
        "By comparing the cross-validation test scores fold-to-fold, count the number of times\n",
        "a random forest is better than a single decision tree.\n",
        "Select the range which this number belongs to:\n",
        "\n",
        "- a) [0, 3]: the random forest model is substantially worse than the single decision tree regressor\n",
        "- b) [4, 6]: both models are almost equivalent\n",
        "- c) [7, 10]: the random forest model is substantially better than the single decision tree regressor\n",
        "\n",
        "_Select a single answer_\n",
        "```\n",
        "\n",
        "\n",
        "Now, train and evaluate with the same cross-validation strategy a random forest\n",
        "with 5 decision trees and another containing 100 decision trees. Once again\n",
        "store the training score.\n",
        "\n",
        "## Question 2\n",
        "```\n",
        "By comparing the cross-validation test scores fold-to-fold, count the number of times\n",
        "a random forest with 100 decision trees is better than a random forest with\n",
        "5 decision trees.\n",
        "Select the range which this number belongs to:\n",
        "\n",
        "- a) [0, 3]: the random forest model with 100 decision trees is substantially worse than the random forest model with 5 decision trees\n",
        "- b) [4, 6]: both models are almost equivalent\n",
        "- c) [7, 10]: the random forest model with 100 decision trees is substantially better than the random forest model with 5 decision trees\n",
        "\n",
        "_Select a single answer_\n",
        "```\n",
        "\n",
        "Plot the validation curve of the `n_estimators` parameters defined by:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "n_estimators = np.array([1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000])\n",
        "```\n",
        "\n",
        "## Question 3\n",
        "\n",
        "```\n",
        "Select the correct statements below.\n",
        "\n",
        "- a) the **train score** decreases when `n_estimators` become large (above 500 trees)\n",
        "- b) the **train score** reaches a plateau when `n_estimators` become large (above 500 trees)\n",
        "- c) the **train score** increases when `n_estimators` become large (above 500 trees)\n",
        "- d) the **test score** decreases when `n_estimators` become large (above 500 trees)\n",
        "- e) the **test score** reaches a plateau when `n_estimators` become large (above 500 trees)\n",
        "- f) the **test score** increases when `n_estimators` become large (above 500 trees)\n",
        "\n",
        "_Select all answers that apply_\n",
        "```\n",
        "\n",
        "Repeat the previous experiment but this time, instead of choosing the default\n",
        "parameters for the random forest, set the parameter `max_depth=5` and build\n",
        "the validation curve.\n",
        "\n",
        "## Question 4\n",
        "```\n",
        "Comparing the validation curve (train and test scores) of the random forest\n",
        "with a full depth and the random forest with a limited depth, select the correct\n",
        "statements.\n",
        "\n",
        "- a) the **test score** of the random forest with a full depth is (almost) always better than the **test score** of the random forest with a limited depth\n",
        "- b) the **train score** of the random forest with a full depth is (almost) always better than the **train score** of the random forest with a limited depth\n",
        "- c) the gap between the train and test scores decreases when reducing the depth of the trees of the random forest\n",
        "- d) the gap between the train and test scores increases when reducing the depth of the trees of the random forest\n",
        "\n",
        "\n",
        "_Select all answers that apply_\n",
        "```\n",
        "\n",
        "\n",
        "Let us now focus at the very beginning of the validation curves, and\n",
        "consider the training score of a random forests with a single tree\n",
        "while using the default `max_depth=None` parameter setting:\n",
        "\n",
        "```python\n",
        "rf_1_tree = RandomForestRegressor(n_estimators=1, random_state=0)\n",
        "cv_results_tree = cross_validate(\n",
        "    rf_1_tree, data, target, cv=10, return_train_score=True\n",
        ")\n",
        "cv_results_tree[\"train_score\"]\n",
        "```\n",
        "\n",
        "should return:\n",
        "\n",
        "```\n",
        "array([0.83120264, 0.83309064, 0.83195043, 0.84834224, 0.85790323,\n",
        "       0.86235297, 0.84791111, 0.85183089, 0.82241954, 0.85045978])\n",
        "\n",
        "```\n",
        "\n",
        "The fact that this single-tree Random Forest can never reach\n",
        "a perfect R2 score of 1.0 on the training can be surprising.\n",
        "\n",
        "Indeed, if you we evaluate the training accuracy of the single\n",
        "`DecisionTreeRegressor` one gets perfect memorization of the\n",
        "training data:\n",
        "\n",
        "```python\n",
        "tree = DecisionTreeRegressor(random_state=0)\n",
        "cv_results_tree = cross_validate(\n",
        "    tree, data, target, cv=10, return_train_score=True\n",
        ")\n",
        "cv_results_tree[\"train_score\"]\n",
        "```\n",
        "\n",
        "which outputs the expected perfect score:\n",
        "\n",
        "```\n",
        "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
        "```\n",
        "\n",
        "## Question 5\n",
        "```\n",
        "From the following statements, select the one that explains\n",
        "that a single-tree random forest cannot achieve perfect\n",
        "training scores.\n",
        "\n",
        "- a) the single tree in the random forest is trained using a bootstrap of the training set and not the training set itself (because `bootstrap=True` by default)\n",
        "- b) for a given feature, the single tree in the random forest uses random splits while the single decision tree uses the best split\n",
        "- c) the random forest automatically limits the depth of the single decision tree, which prevents overfitting\n",
        "\n",
        "_Select a single answer_\n",
        "```\n",
        "\n",
        "Build a validation curve for a `sklearn.ensemble.HistGradientBoostingRegressor`\n",
        "varying `max_iter` as follows:\n",
        "\n",
        "```python\n",
        "max_iters = np.array([1, 2, 5, 10, 20, 50, 100, 200, 500])\n",
        "```\n",
        "\n",
        "We recall that `max_iter` corresponds to the number of trees in the boosted\n",
        "model.\n",
        "\n",
        "Plot the average train and test score for each value of `max_iter`.\n",
        "\n",
        "## Question 6\n",
        "```\n",
        "Select the correct statements.\n",
        "\n",
        "- a) for a small number of trees (between 5 and 10 trees), the gradient boosting model behave like the random forest algorithm: the train scores are high while the test scores are not optimum\n",
        "- b) for a small number of trees (between 5 and 10 trees), the gradient boosting model behave differently to the random forest algorithm: both the train and test scores are small\n",
        "- c) with a large number of trees (> 100 trees) adding more trees in the ensemble causes the gradient boosting model overfit (increasing the gap between the train score and  test score)\n",
        "- d) with a large number of trees (> 100 trees) adding more trees in the ensemble does not impact the generalization performance of the gradient boosting model\n",
        "\n",
        "_Select all answers that apply_\n",
        "```"
      ],
      "metadata": {
        "id": "8ysaUIsCfGEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###QUESTION 1\n",
        "\n",
        "tree = DecisionTreeRegressor(random_state=0)\n",
        "rf_default = RandomForestRegressor(random_state=0)\n",
        "\n",
        "cv_results_tree = cross_validate(tree, data, target, cv=10,scoring=\"r2\", return_train_score=True)\n",
        "cv_results_rf_default = cross_validate(rf_default, data, target, cv=10, scoring=\"r2\", return_train_score=True)\n",
        "\n",
        "tree_scores = cv_results_tree['test_score']\n",
        "rf_default_scores = cv_results_rf_default['test_score']\n",
        "\n",
        "\n",
        "\n",
        "print(\"Scores R2 de la Forêt Aléatoire \",rf_default_scores)\n",
        "print(\"Scores R2 de l'Arbre de Décision \",tree_scores)\n",
        "\n",
        "tree_train_scores = cv_results_tree['train_score']\n"
      ],
      "metadata": {
        "id": "U4bRBqsdVrY0",
        "outputId": "ecda5296-f941-4cbf-c424-e1bd7ffbe213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores R2 de la Forêt Aléatoire  [0.78812533 0.77436104 0.88871396 0.84661427 0.78043644 0.85771461\n",
            " 0.79185662 0.73543268 0.76485281 0.8077531 ]\n",
            "Scores R2 de l'Arbre de Décision  [0.59650558 0.66957713 0.7193112  0.8184209  0.6242818  0.72483157\n",
            " 0.53090702 0.51655398 0.58421785 0.49038086]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La Forêt Aléatoire gagne 10 fois sur 10.\n",
        "\n",
        "Réponse Q1: c)"
      ],
      "metadata": {
        "id": "2USHB08-WOcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Question 2\n",
        "rf_5 = RandomForestRegressor(n_estimators=5, random_state=0)\n",
        "cv_results_rf_5 = cross_validate(rf_5, data, target, cv=10,scoring=\"r2\", return_train_score=True)\n",
        "rf_5_scores = cv_results_rf_5['test_score']\n",
        "\n",
        "###par défaut le nombre d'arbres est réglée sur 100 donc il suffit de comparer avec le précédent\n",
        "print(rf_default_scores, rf_5_scores)"
      ],
      "metadata": {
        "id": "YXSxETPdfuxF",
        "outputId": "a2a42ade-e331-47cb-ab57-2fab5fbc7752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.78812533 0.77436104 0.88871396 0.84661427 0.78043644 0.85771461\n",
            " 0.79185662 0.73543268 0.76485281 0.8077531 ] [0.77235183 0.65802718 0.85849519 0.82029064 0.78921964 0.85126513\n",
            " 0.77881189 0.6144532  0.76453205 0.7611934 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La RF (100 arbres) gagne 9 fois sur 10 vs RF (5 arbres).\n",
        "\n",
        "Réponse Q2: c)"
      ],
      "metadata": {
        "id": "yUk4Hg11XaIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###question 3\n",
        "import numpy as np\n",
        "n_estimators = np.array([1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000])\n",
        "\n",
        "train_scores_default, test_scores_default = validation_curve(\n",
        "    RandomForestRegressor(random_state=0),\n",
        "    data, target,\n",
        "    param_name=\"n_estimators\",\n",
        "    param_range=n_estimators,\n",
        "    cv=10,\n",
        "    scoring=\"r2\",\n",
        ")\n",
        "\n",
        "default_train_mean = np.mean(train_scores_default, axis=1)\n",
        "default_test_mean = np.mean(test_scores_default, axis=1)\n",
        "\n",
        "print(\"N_estimators\", n_estimators)\n",
        "print(\"Mean Train Scores\", default_train_mean)\n",
        "print(\"Mean Test Scores\", default_test_mean)\n",
        "\n"
      ],
      "metadata": {
        "id": "e39V9MndYPnw",
        "outputId": "6f66732d-9a63-4bd5-9a2c-fde43e339e02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_estimators [   1    2    5   10   20   50  100  200  500 1000]\n",
            "Mean Train Scores [0.84374635 0.91509573 0.94981818 0.95903036 0.96706578 0.97166572\n",
            " 0.97228981 0.97307673 0.97364496 0.97363665]\n",
            "Mean Test Scores [0.5938381  0.701387   0.76686401 0.78438411 0.79013243 0.80146742\n",
            " 0.80358609 0.80521083 0.80414542 0.8051176 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les scores atteignent un plateau"
      ],
      "metadata": {
        "id": "MHvQrg_dbR6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###QUESTION 4\n",
        "\n",
        "train_scores_5, test_scores_5 = validation_curve(\n",
        "    RandomForestRegressor(random_state=0, max_depth=5),\n",
        "    data, target,\n",
        "    param_name=\"n_estimators\",\n",
        "    param_range=n_estimators,\n",
        "    cv=10,\n",
        "    scoring=\"r2\",\n",
        ")\n",
        "\n",
        "depth5_train_mean = np.mean(train_scores_5, axis=1)\n",
        "depth5_test_mean = np.mean(test_scores_5, axis=1)\n",
        "\n",
        "print(\"N_estimators\", n_estimators)\n",
        "print(\"Mean Train Scores\", depth5_train_mean)\n",
        "print(\"Mean Test Scores\", depth5_test_mean)\n"
      ],
      "metadata": {
        "id": "nzdCk2BUbWmZ",
        "outputId": "dd1cddc6-a1a9-49f7-fd8d-9ce43c8d689c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_estimators [   1    2    5   10   20   50  100  200  500 1000]\n",
            "Mean Train Scores [0.82363417 0.86646152 0.88696601 0.8950874  0.89969725 0.90323255\n",
            " 0.90395147 0.90480355 0.90517693 0.90515391]\n",
            "Mean Test Scores [0.71462582 0.75665702 0.77816612 0.79305879 0.80212391 0.81081494\n",
            " 0.81393895 0.81617998 0.81582331 0.8164125 ]\n"
          ]
        }
      ]
    }
  ]
}